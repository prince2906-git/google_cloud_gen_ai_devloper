## **History Of AI at Google**

### 2001: Machine learning helps Google Search users correct their spelling
    A simple version of machine learning to suggest better spellings for web searches. 
    Even if you don’t type it perfectly, Google Search can still get you what you need.

### 2006: Google Translate launches
    Google Translate, which used machine learning to automatically translate languages. 
    It started with Arabic to English and English to Arabic translations, but today Google Translate supports 133 languages spoken by millions of people around the world. 
    This technology can translate text, images or even a conversation in real time, breaking down language barriers across the global community,
    helping people communicate and expanding access to information like never before.
    

### 2015: TensorFlow democratizes AI
    The introduction of TensorFlow, a new open source machine learning framework, made AI more accessible, scalable and efficient. 
    It also helped accelerate the pace of AI research and development around the world. TensorFlow is now one of the most popular machine learning frameworks, 
    and has been used to develop a wide range of AI applications, from image recognition to natural language processing to machine translation.

### 2016: AlphaGo defeats world champion Go player
    As part of the Google DeepMind Challenge Match, more than 200 million people watched online as AlphaGo became the first AI program to defeat a human world champion in Go.
    A complex board game previously considered out of reach for machines. This milestone victory demonstrated deep learning's potential to solve complex problems once thought impossible for computers. 
    AlphaGo's victory over Lee Sedol, one of the world's best Go players, sparked a global conversation about AI's future and showed that AI systems could now learn to master complex games requiring strategic thinking and creativity.

### 2016: TPUs enable faster, more efficient AI deployment
    Tensor Processing Units, or TPUs, are custom-designed silicon chips we specifically invented for machine learning and optimized for TensorFlow. 
    They can train and run AI models much faster than traditional chips, which makes them ideal for large-scale AI applications. 
    Version v5e, announced in August, is the most cost-efficient, versatile, and scalable Cloud TPU to date.

### 2017: Google Research introduces the Transformer
    The Google Research paper “Attention Is All You Need” introduced the Transformer, a new neural network architecture that helped with language understanding. 
    Before the Transformer, machines were not very good at understanding the meaning of long sentences — they couldn’t see the relationships between words that were far apart. 
    The Transformer hugely improved this and has become the bedrock of today’s most impressive language understanding and generative AI systems. 
    The Transformer has revolutionized what it means for machines to perform translation, text summarization, question answering and even image generation and robotics.

### 2019: BERT helps Search better understand queries
    Google's research on Transformers led to the introduction of Bidirectional Encoder Representations from Transformers, or BERT for short, which helped Search understand users’ queries better than ever before. 
    Rather than aiming to understand words individually, BERT algorithms helped Google understand words in context. This led to a huge quality improvement across Search, and made it easier for people to ask questions as they naturally would, 
    rather than by stringing keywords together.

### 2020: AlphaFold solves the protein-folding problem
    In 2020, DeepMind made a leap in the field of AI with its system, AlphaFold, which was recognized as a solution to the “protein-folding problem.” 
    Proteins are the building blocks of life, and the way a protein folds determines its function; a misfolded protein could cause disease. 
    For 50 years, scientists had been trying to predict how a protein would fold to help understand and treat diseases. 
    AlphaFold did just that. Then, in 2022, we shared 200 million of AlphaFold’s protein structures — covering almost every organism on the planet that has had its genome sequenced — freely with the scientific community via the AlphaFold Protein Structure Database. 
    More than 1 million researchers have already used it to work on everything from accelerating new malaria vaccines in record time to advancing cancer drug discovery and developing plastic-eating enzymes.

### 2023: Bard helps you collaborate with generative AI
    LaMDA, a conversational large language model released by Google Research in 2021, paved the way for many generative AI systems that have captured the world’s imagination, including Bard. 
    Launched in March, Bard is now available in most of the world and in over 40 languages, so more people than ever can use it to boost productivity, accelerate ideas and fuel curiosity. 
    And we’ve combined Bard’s smartest and most capable model yet with the Google services you use every day — like Gmail, Docs, Drive, Flights, Maps and YouTube — to be even more helpful for you with tasks like trip planning, double-checking answers and summarizing emails or documents.

### 2023: PaLM 2 advances the future of AI
    This May, we introduced PaLM 2, our next generation large language model that has improved multilingual, reasoning and coding capabilities.
    It’s more capable, faster and more efficient than its predecessors, and is already powering more than 25 Google products and features — including Bard, generative AI features in Gmail and Workspace, and SGE, Google's experiment to deeply integrate generative AI into Google Search.
    We’re also using PaLM 2 to advance research internally on everything from healthcare to cybersecurity.
